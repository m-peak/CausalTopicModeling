{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mvp = pd.read_csv('%s/%s' % ('mvp.csv'),\n",
    "                  usecols=['id', 'name', 'team', 'league', 'year'],\n",
    "                  dtype={'id': int, 'name': str, 'team': str, 'league': str, 'year': int})\n",
    "\n",
    "article_labeled = pd.read_csv('%s/%s' % ('article_labeled.csv'),\n",
    "                  usecols=['id', 'year', 'text', 'name'],\n",
    "                  dtype={'id': int, 'year': int, 'text': str, 'name': str})\n",
    "\n",
    "# Stats: Number of Articles per Year\n",
    "article_labeled.groupby(['year']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5761ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 1-4. Word Processing\n",
    "###################################\n",
    "import nltk\n",
    "nltk.download('wordnet') # for lemmatizer\n",
    "import gensim\n",
    "stop_words = gensim.parsing.preprocessing.STOPWORDS.union(set(['mlb', 'major', 'league', 'baseball', 'game', 'team']))\n",
    "\n",
    "MIN_COUNT = 1\n",
    "THRESHOLD = 1 # smaller: longer phrases (e.g. Los Angeles Angeles)\n",
    "    \n",
    "# Create BagOfWords\n",
    "def create_bow(values):\n",
    "    # Tokenize Text\n",
    "    docs_tokenized = [val.split() for val in values]\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    docs_sw_removed = [[token for token in doc if token not in stop_words] for doc in docs_tokenized]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    docs_lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs_sw_removed]\n",
    "\n",
    "    # Snowball English Stemming\n",
    "    stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "    docs_stemmed = [[stemmer.stem(token) for token in doc] for doc in docs_lemmatized]\n",
    "    return docs_stemmed\n",
    "\n",
    "###################################\n",
    "# 1-5. Bigram & Trigram Models\n",
    "###################################\n",
    "# Create Bigram Model\n",
    "def create_bigram_model(docs):\n",
    "    bigram = gensim.models.Phrases(docs, min_count=MIN_COUNT, threshold=THRESHOLD)\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_model, bigram\n",
    "\n",
    "# Create Trigram Model\n",
    "def create_trigram_model(docs, bigram):\n",
    "    trigram = gensim.models.Phrases(bigram[docs], min_count=MIN_COUNT, threshold=THRESHOLD)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "    return trigram_model\n",
    "\n",
    "# Create Bigram Trigram Models\n",
    "def create_bigram_trigram_models(values):\n",
    "    # BagOfWords\n",
    "    docs = create_bow(values)\n",
    "    \n",
    "    # Bigram Model\n",
    "    bigram_model, bigram = create_bigram_model(docs)\n",
    "    \n",
    "    # Trigram Model\n",
    "    trigram_model = create_trigram_model(docs, bigram)\n",
    "    return bigram_model, trigram_model\n",
    "\n",
    "###################################\n",
    "# 1-6. Bigram & Trigram Terms\n",
    "###################################\n",
    "# Create Bigram Terms\n",
    "def create_bigram_terms(bigram_model, docs):\n",
    "    bigram_terms = [bigram_model[doc] for doc in docs]\n",
    "    return bigram_terms\n",
    "\n",
    "# Create Trigram Terms\n",
    "def create_trigram_terms(bigram_model, trigram_model, docs):\n",
    "    bigram_terms = create_bigram_terms(bigram_model, docs)\n",
    "    trigram_terms = [trigram_model[doc] for doc in bigram_terms]\n",
    "    return trigram_terms\n",
    "\n",
    "# Create Trigram\n",
    "def create_Trigram(values, bigram_model, trigram_model):\n",
    "    # BagOfWords\n",
    "    docs = create_bow(values)\n",
    "    \n",
    "    # Trigram Terms\n",
    "    trigram_terms = create_trigram_terms(bigram_model, trigram_model, docs)\n",
    "    return trigram_terms\n",
    "\n",
    "###################################\n",
    "# 1-7. Dictionary & Corpus Creation\n",
    "###################################\n",
    "# Create Dictionary: {TermID: Term} (e.g. {0: 'apple'})\n",
    "def create_dictionary(trigram_terms):\n",
    "    dictionary = gensim.corpora.Dictionary(trigram_terms)\n",
    "    return dictionary\n",
    "\n",
    "# Create Corpus: (TermID, Frequency) (e.g. [(0, 1), (1, 2), ...])\n",
    "def create_corpus(trigram_terms, dictionary):\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in trigram_terms]\n",
    "    return corpus\n",
    "\n",
    "# Create Dictionary & Corpus\n",
    "def create_dict_corpus(trigram_terms):\n",
    "    dictionary = create_dictionary(trigram_terms)\n",
    "    corpus = create_corpus(trigram_terms, dictionary)\n",
    "    return dictionary, corpus, trigram_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 1-8. lemmatizer & Stemmer Test\n",
    "###################################\n",
    "pd.set_option('display.max_columns', None) # None=auto detect\n",
    "\n",
    "# Lemmatizer\n",
    "sample_lemmatized1 = ['Lemmatizer'] + [lemmatizer.lemmatize(token) for token in sample_swr1]\n",
    "\n",
    "# Snowball English Stemmer\n",
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "# Porter Stemmer\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# Lancaster Stemmer\n",
    "lancaster = nltk.stem.lancaster.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 1-9. lemmatizer & Stemmer Test1\n",
    "###################################\n",
    "sample_text1 = article_labeled.text.values[101].split()\n",
    "print(' '.join(sample_text))\n",
    "\n",
    "# Stop Words Removal\n",
    "sample_swr1 = [token for token in sample_text1 if token not in stop_words]\n",
    "\n",
    "sample_snowball1 = ['Snowball'] + [snowball.stem(token) for token in sample_swr1]\n",
    "sample_porter1 = ['Porter'] + [porter.stem(token) for token in sample_swr1]\n",
    "sample_lancaster1 = ['Lancaster'] + [lancaster.stem(token) for token in sample_swr1]\n",
    "\n",
    "pd.DataFrame([sample_lemmatized1, sample_snowball1, sample_porter1, sample_lancaster1], columns=[''] + sample_swr1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlb",
   "language": "python",
   "name": "mlb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
